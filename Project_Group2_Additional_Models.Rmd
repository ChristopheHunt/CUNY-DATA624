---
title: "Project_Group2_Additional_Models"
author: "Daniel Hong"
date: "November 26, 2017"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(caret)
library(mlbench)
library(MASS)
library(AppliedPredictiveModeling)
library(lars)
library(pls)
library(elasticnet)
library(rpart)
library(e1071)
```

```{r cache=TRUE}
dfModImp <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/CUNY-DATA624/master/data/TrainImputeData.csv")
dfPredImp <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/CUNY-DATA624/master/data/PredictImputeData.csv")
```

PLS
```{r cache=TRUE}
set.seed(1234)
plsFit = plsr(PH ~ ., data=dfModImp, validation="CV")
pls.pred = predict(plsFit, dfPredImp[1:5, ], ncomp=1:2)

pls.pred

validationplot(plsFit, val.type="RMSEP")
validationplot(plsFit, val.type="R2")

pls.RMSEP = RMSEP(plsFit, estimate="CV")
plot(pls.RMSEP, main="RMSEP PLS PH", xlab="Components")
min_comp = which.min(pls.RMSEP$val)
points(min_comp, min(pls.RMSEP$val), pch=1, col="red", cex=1.5)

min_comp

plot(plsFit, ncomp=11, asp=1, line=TRUE)

pls.pred2 = predict(plsFit, dfPredImp, ncomp=11)
pls.pred2
```

```{r}
#10 fold cross validation, not sure if we should use repeated cv here but part of stackoverlow post: Repeating a crossvalidation with exactly the same splitting will yield exactly the same result for every repetition
trainControl <- trainControl(method = "cv", number = 10)

#GLM
set.seed(1234)
fit.glm <- train(PH~., data=dfModImp, method="glm", metric="RMSE", trControl=trainControl,
           tuneLength = 5, preProc = c("center", "scale"))
fit.glm

#SVM
set.seed(1234)
fit.svm <- train(PH~., data=dfModImp, method="svmLinear3", metric="RMSE", trControl=trainControl,                               tuneLength = 5, svr_eps = .1, preProc = c("center", "scale"))
fit.svm

#KNN
set.seed(1234)
fit.knn <- train(PH~., data=dfModImp, method="knn", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.knn

#CART
set.seed(1234)
fit.cart <- train(PH~., data=dfModImp, method="rpart", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.cart

#Bagged CART
set.seed(1234)
fit.bagcart <- train(PH~., data=dfModImp, method="treebag", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.bagcart

#We looked at GBM on the other dataset but wanted to include here to show the comparison
set.seed(1234)
fit.gbm <- train(PH~., data=dfModImp, method="gbm", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.gbm

# We also looked at Random Forest on the other dataset, took too long to run here, quit iteration
set.seed(1234)
fit.rf <- train(PH~., data=dfModImp, method="cforest", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.rf

#Bagged Mars took to long to run as well, quit before results came up.
set.seed(1234)
fit.bagmars <- train(PH~., data=dfModImp, method="bagEarth", metric="RMSE", trControl=trainControl,
          tuneLength = 5, preProc = c("center", "scale"))
fit.bagmars

#Compare results of the algorithms we ran
results <- resamples(list(GLM=fit.glm, SVM=fit.svm, KNN=fit.knn, CART=fit.cart, BaggedCart=fit.bagcart, GBM=fit.gbm))

summary(results)
```