---
title: "Data624 - Project2"
author: "Group 2"
date: "November 13, 2017"
output: html_document
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, message=FALSE, warning=FALSE)
```

#Data Exploration
We will load the data into a dataframe and get started

```{r cache=TRUE}
library(missForest)
library(corrgram)
library(caret)
library(psych)
library(knitr)

dfBevMod <- read.csv("https://github.com/ChristopheHunt/CUNY-DATA624/raw/master/data/StudentData.csv", header = TRUE)
dfBevPred <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/CUNY-DATA624/master/data/StudentEvaluation-%20TO%20PREDICT.csv", header =TRUE)
```
First we will examine the training dataset we want to see how many predictor variables we are dealing with and if we are missing any data. We see several variables with missing data, 

```{r cache=TRUE}
dim(dfBevMod)
```
Looks like we have a total of 32 predictor variables and a target variable. Next we will check for any missing data.

```{r cache=TRUE}
colSums(is.na(dfBevMod))
```
We see many variables with NA's -- notably "MFR" has 212. Still, roughly 8% NA is workable, so we'll choose to impute, later. 

Also, it appears that we have one categorical variable: Brand.Code

###Barchart
```{r cache=TRUE}
#Visualizing the single categorical variable
barchart(dfBevMod[,1], col="Gold")
```
It appears that brand "B" occurs most frequently, followed by "D"

### Continuous and Discrete variables
```{r}
table.desc <- describe(dfBevMod[,-1])
table.prep <- as.matrix(table.desc)
table.round <- round((table.prep), 2)
kable(table.round)
```


###Nominal Variable Histogram
```{r cache=TRUE}
dfBevModH <- dfBevMod[2:ncol(dfBevMod)] #removing factor var
par(mfrow = c(3,5), cex = .5)
for(i in colnames(dfBevModH)){
hist(dfBevModH[,i], xlab = names(dfBevMod[i]),
  main = names(dfBevModH[i]), col="grey", ylab="")
}
```
Mnf.Flow and Hyd.Pressure 1,2,3 each have many values below 0 -- possibly null-type entered values. Several variables are strongly skewed -- some of which appear to have outliers. 

Exploring other variables, we see 
```{r cache=TRUE}
BrandA <- dfBevMod[dfBevMod$Brand.Code == "A",]
summary(dfBevMod[dfBevMod$Mnf.Flow == -100,])
plot(dfBevMod$Brand.Code, dfBevMod$Mnf.Flow)
table(dfBevMod$Brand.Code, dfBevMod$Mnf.Flow)
nrow(dfBevMod[dfBevMod$Mnf.Flow < 1,])
```

###Density Plot
```{r cache=TRUE}
par(mfrow = c(3,5), cex = .5)
for (i in colnames(dfBevModH)) {
 smoothScatter(dfBevModH[,i], main = names(dfBevModH[i]), ylab = "", 
   xlab = "", colramp = colorRampPalette(c("white", "red")))
 }
```
The odd data in Mnf.Flow appears to be related to similarly zero-out data in Hyd.Pressure1, Hyd.Press2, and Hyd.Pressure3. Several other variables have dichotomous patterns in data behavior, including "Carb.Pressure1", "Filler.Level", "Usage.cont", "Carb.Flow", and "Oxygen.Filler". This leaves two options; we can alter these gaps by possibly inputing new values in, or we can use algorithms that can easily handle quick pattern shifts, such as forests and MARS.  

###BoxPlots
```{r cache=TRUE}
par(mfrow = c(3,5), cex = .5)
for(i in colnames(dfBevModH)){
boxplot(dfBevModH[,i], xlab = names(dfBevModH[i]),
  main = names(dfBevModH[i]), col="grey", ylab="")
}
```
Again, several variables with large skews and outliers

###Principle Component Analysis
```{r cache=TRUE, warning=F}
PCA <- function(X) {
  Xpca <- prcomp(na.omit(X), center = T, scale. = T) 
  M <- as.matrix(na.omit(X)); R <- as.matrix(Xpca$rotation); score <- M %*% R
  print(list("Importance of Components" = summary(Xpca)$importance[ ,1:5], 
             "Rotation (Variable Loadings)" = Xpca$rotation[ ,1:5],
             "Correlation between X and PC" = cor(na.omit(X), score)[ ,1:5]))
  par(mfrow=c(2,3))
  barplot(Xpca$sdev^2, ylab = "Component Variance")
  barplot(cor(cbind(X)), ylab = "Correlations")
  barplot(Xpca$rotation, ylab = "Loadings")  
  biplot(Xpca); barplot(M); barplot(score)
}
PCA(dfBevModH)
```
First two components account for most of the variance, although Mnf.Flow is highly prioritized, so I'm concerned that it may be a function of the null-like values.  

#Data Transformation

Transform categorical variable Brand.Code
```{r cache=TRUE}
#df_imputed$Brand.Code = NULL
dfBevMod$Brand.Code[dfBevMod$Brand.Code == ""] <- NA
dfBevMod$Brand.Code <- droplevels(dfBevMod$Brand.Code)

dfBevPred$Brand.Code[dfBevPred$Brand.Code == ""] <- NA
dfBevPred$Brand.Code <- droplevels(dfBevPred$Brand.Code)

#Recode categorical factor
dfBevMod$A <- ifelse(dfBevMod$Brand.Code == "A", 1, 0)
dfBevMod$B <- ifelse(dfBevMod$Brand.Code == "B", 1, 0)
dfBevMod$C <- ifelse(dfBevMod$Brand.Code == "C", 1, 0)
dfBevMod$D <- ifelse(dfBevMod$Brand.Code == "D", 1, 0)
dfBevMod$Brand.Code <- NULL

dfBevPred$A <- ifelse(dfBevPred$Brand.Code == "A", 1, 0)
dfBevPred$B <- ifelse(dfBevPred$Brand.Code == "B", 1, 0)
dfBevPred$C <- ifelse(dfBevPred$Brand.Code == "C", 1, 0)
dfBevPred$D <- ifelse(dfBevPred$Brand.Code == "D", 1, 0)
dfBevPred$Brand.Code <- NULL
```

###Imputation
Next we will use the missForest library to impute the missing variable of the predictor variables
```{r cache=TRUE}
#dfImpMod = missForest(dfBevMod)
#dfImpMod$OOBerror #error rate looks good?
#dfModImp <- dfImpMod$ximp

#dfImpPred <- missForest(dfBevPred)
#dfPredImp <- dfImpPred$ximp
#dfPredImp$PH <- NA #redadding PH

#write.csv(dfModImp, "TrainImputeData.csv")
#write.csv(dfPredImp, "PredictImputeData.csv")

#Stored current imputation results on github to quicken knitr iterations
dfModImp <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/CUNY-DATA624/master/data/TrainImputeData.csv")
dfPredImp <- read.csv("https://raw.githubusercontent.com/ChristopheHunt/CUNY-DATA624/master/data/PredictImputeData.csv")
```

###Preprocessing
Due to the different types of model imputs (some preprocess, other's dont), creating a range of preprocessed variables
```{r cache=TRUE}

dfModImpX <- dfModImp[,!(names(dfModImp) == "PH")]
dfModImpY <- dfModImp[, names(dfModImp) == "PH"]

dfPredImpX <- dfPredImp[,!(names(dfPredImp) == "PH")]
dfPredImpY <- dfPredImp[, names(dfPredImp) == "PH"]

#Spatial Sign outlier processing
dfModImpSsX <- spatialSign(dfModImpX)
dfPredImpSsX <- spatialSign(dfPredImpX)

#BoxCox Only
transModB <- preProcess(dfModImpSsX, method = "BoxCox") #transformed all 22 variables
dfModBX <- predict(transModB, dfModImpSsX)

transPredB <- preProcess(dfPredImpSsX, method = "BoxCox") #transformed 23 variables (should we use the Modeling model from above, instead of predicting model?)
dfPredBX <- predict(transPredB, dfPredImpSsX)

#BoxCox, Centering, and Scaling
transModBCS <- preProcess(dfModImpSsX, method = c("BoxCox", "center", "scale")) #22 BC, 35 centered, 35 scaled
dfModBCSX <- predict(transModBCS, dfModImpSsX)

transPredBCS <- preProcess(dfPredImpSsX, method = c("BoxCox", "center", "scale")) #23 BC, 35 centered, 35 scaled
dfPredBCSX <- predict(transPredBCS, dfPredImpSsX)

#BoxCox, Centering, Scaling, and PCA
transModBCSP <- preProcess(dfModImpSsX, method = c("BoxCox", "center", "scale", "pca")) #22 BC, 35 centered, 35 scaled
dfModBCSPX <- predict(transModBCSP, dfModImpSsX)

transPredBCSP <- preProcess(dfPredImpSsX, method = c("BoxCox", "center", "scale", "pca")) #23 BC, 35 centered, 35 scaled
dfPredBCSPX <- predict(transPredBCSP, dfPredImpSsX)
```
Attempt to reduce the number of predictor variables. We will review the correlation between the variables and find the highly correlated ones that can be reduced.
```{r cache=TRUE}
#corrgram(dfModBCSX, order=TRUE,
#         upper.panel=panel.cor, main="Correlation Matrix")
library(corrplot)
install.packages("corrplot")
correlations <- cor(dfModBCSX)
corrplot(correlations, order = "hclust", tl.cex = 0.55)
```
Several very highly correlated variables.

Next we will reduce our dataset and remove pairs that have correlation above 0.75
```{r cache=TRUE}
hc = findCorrelation(correlations, cutoff=0.75)
length(hc) #18 vars

#Reducing
dfModBCSRX = dfModBCSX[,-c(hc)] #Box-Cox, Center, Scale
dfPredBCSRX = dfPredBCSX[,-c(hc)]

dfModBRX = dfModBX[,-c(hc)] #Box-Cox
dfPredBRX = dfPredBCSX[,-c(hc)]

dfModSRX = dfModImpSsX[,-c(hc)] #Only Spatial Sign
dfPredSRX = dfPredImpSsX[,-c(hc)]
```

Ready variables for modelling
```{r cache=TRUE}
set.seed(2017)
n75 <- floor(0.75 * nrow(dfBevMod)) #75$ of sample size
n <- sample(seq_len(nrow(dfBevMod)), size = n75)

#Box-Cox, Center, Scale
dfTrainBCSX <- dfModBCSRX[n,]
dfTestBCSX <- dfModBCSRX[-n,]

#Box-Cox
dfTrainBX <- dfModBX[n,]
dfTestBX <- dfModBX[-n,]

#Only Spatial Sign
dfTrainX <- dfModSRX[n,]
dfTestX <- dfModSRX[-n,]

#Response variable
dfTrainY <- dfModImpY[n]
dfTestY <- dfModImpY[-n]
```